##### 玻尔兹曼机（RBM）

* 递归神经网络的本质是学习一个函数，因此有输入和输出层的概念，而玻尔兹曼机的用处在于学习一组数据的内在表示，没有输出层的概念
* 递归神经网络各个节点链接为有向环，而玻尔兹曼机各节点连接成**无向完全图** 
* 玻尔兹曼机也有隐藏层和输入层，但是没有输出层
* 受限玻尔兹曼机和玻尔兹曼机相比，主要就是将完全图变为了二分图，可以用来降维，学习特征，深度信念网络等

##### Hopfield网络以及能量函数

是一种循环型神经网络，当信号输入之后会最后趋于稳定或者呈现周期性震荡。

假设神经元$i$ 连接其他神经元$j$ 的权重为$w(i,j)$ ，则在Hopfield中，会有$w(i,i)=0$ ，即神经元不与自己连接，而且$w(i,j) = w(j,i)$ 

假设每一轮$t$ 中，神经元$i$ 的状态用$y(i,t)$ 来表示，并且假设神经元的激活函数为$sigmod$ ，并且激发界限用$t(i)$ 来表示那么可以得到以下的递推式：

$y(i,0) = sigmod(a_i-t(i))$     $y(j,t+1) = sigmod(\sum\limits_{i=1}^N w_{ij}y(i,t) +X_j - \theta_j)$ 

$X_j$ 在这里是表示输入的。

由此定义能量的增量$\Delta E_j = -[\sum\limits_{ \begin{align}  & j=1 \\& i \neq j\end{align}}^nw_{ij}y_i+X_j - \theta_j] \Delta y_j$

RBM是一个基于能量的模型，首先需要定义能量函数。对于一组给定的状态$(v,h)$ ：

$E_{\theta}(v,h) = -\sum\limits_{i=1}^{n_v} a_i v_i - \sum\limits_{j=1} ^{n_h} b_j h_j -\sum\limits_{i=1} ^{n_v} \sum\limits_{j=1}^{n_h} h_j w_{j,i} v_i$

也就是$E_{\theta}(v,h) = -a^T v - b^T h - h^T Wv$ 

可以给出状态$(v,h)$ 的联合概率分布：$P_{\theta}(v,h) = \frac{1}{Z_{\theta}} e^{-E_{\theta}(v,h)},Z_{\theta}=\sum\limits_{v,h} e^{-E_{\theta}(v,h)}$

我们关系的是实际观测数据$v$ 的概率分布$P_{\theta}(v)$ ，对应于$P_{\theta}(v,h)$ 的边缘分布，具体就是$P_{\theta}(v) = \frac{1}{Z_{\theta}} \sum\limits_h e^{-E_{\theta}(v,h)}$ 

将下标$\theta$ 忽略。

$P(h_k=1|v)=sigmod(b_k+ \sum\limits_{i=1}^{n_v} w_{k,i}v_i)$ 





##### RBM训练算法

1. 初始化

   （1） 给定训练样本集合$S(|S| =n_s)$ 

   （2） 给定训练周期$J$ ，学习率$\eta$ ，以及$CD-k$ 算法参数$k$ 

   （3）指定可见层和隐藏层的单元数目$n_v,n_h$ 

   （4）初始化偏置向量$a,b$ 和权重矩阵$W$ 

2. $For \quad iter = 1,2,…,J  Do$ 

   {

   1. 调用$CD-k(k,S,RBM(W,a,b); \Delta W, \Delta a, \Delta b)$ ，生成$(\Delta W , \Delta a , \Delta b)$ 
   2. 刷新参数：$W = W+\eta(\frac{1}{n_s}\Delta W) , a = a+\eta(\frac{1}{n_s}\Delta a) , b = b+\eta(\frac{1}{n_s}\Delta b)$ 

   }

   ​



