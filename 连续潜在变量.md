我们讨论了具有离散潜在变量的概率模型，例如⾼斯混合模型。我们现在研究某些潜在变量或者全部潜在变量为连续变量的模型。研究这种模型的⼀个重要的动机是许多数据集具有下⾯的性质：数据点⼏乎全部位于⽐原始数据空间的维度低得多的流形中。

在这个例⼦中，平移和旋转变量是潜在变量，因为我们值观测到图像向量，不知道创建它们所使⽤的平移或者旋转变量。

在实际应⽤中，数据点不会被精确限制在⼀个光滑的低维流形中，我们可以将数据点关于流形的偏移看做噪声。这就⾃然地引出了这种模型的⽣成式观点，其中我们⾸先根据某种潜在变量的概率分布在流形中选择⼀个点，然后通过添加噪声的⽅式⽣成观测数据点。

#### 主成分分析

主成分分析寻找⼀个低维空间，被称为主⼦平⾯，⽤紫⾊的线表⽰，使得数据点在⼦空间上的正交投影能够最⼤化投影点的⽅差。它也可以被定义为使得平均投影代价最⼩的线性投影。平均投影代价是指数据点和它们的投影之间的平均平⽅距离。

那么最⼤化投影数据⽅差的最优线性投影由数据协⽅差矩阵S的M个特征向量$u_1, \dots, u_M$定义，对应于M个最⼤的特征值$\lambda_1, \dots, \lambda_M$。主成分分析的另⼀个应⽤是数据预处理。在这种情况下，⽬标不是维度降低，⽽是对数据集进⾏变换，使得数据集的某些属性得到标准化。然⽽，使⽤PCA，我们可以对数据进⾏更显著的归⼀化，得到零均值和单位⽅差的数据。

PCA是⽆监督的，值依赖于$x_n$的值，⽽Fisher线性判别分析还使⽤了类别标签的信息。

**高维PCA**

总结⼀下，为了应⽤这种⽅法，我们⾸先计算$XX^T$ ，然后找到它的特征向量和特征值。

**概率PCA**

z上的先验概率分布是一个单均值单位协方差的高斯分布：
$$
p(z) =N(z|0, I)
$$
以潜在变量z的值为条件，观测变量x的条件概率分布还是⾼斯分布，形式为:
$$
p(x|z) = N(x| Wz +\mu, \sigma^2 I)
$$
其中观测值的⼀个采样值通过下⾯的⽅式获得：⾸先为潜在变量选择⼀个值，然后以这个潜在变量的值为条件，对观测变量采样。

#### PCA和EM算法

正如我们已经看到的那样，概率PCA模型可以根据连续潜在空间z上的积分或求和来表⽰，其中对于每个数据点$x_n$，都存在⼀个对应的潜在变量$z_n$。于是，我们可以使⽤EM算法来找到模型参数。概 率PCA的EM算 法 的 执 ⾏ 过 程 为： 对 参 数 进 ⾏ 初 始 化， 然 后 交 替 地 在E步 骤 中 使 ⽤ 公 式（12.54）和（12.55）计算潜在空间的后验概率分布的充分统计量，以及在M步骤中使⽤公式（12.56）和（12.57）来修正参数的值。

#### ⾮线性隐含变量模型

我们将注意⼒集中与带有连续潜在变量的最简单的⼀类模型上，即基于线性⾼斯分布的模型。

**独立成分分析**

其中我们忽略信号的时序本质，将连续的样本看成是独⽴同分布的。我们考虑⼀个⽣成式模型，其中有两个潜在变量，对应于未观测的语⾳信号的幅值，有两个观测变量，由麦克风的信号值给定。