#### 参数最优化：

我们可以考虑寻找使得选定的误差函数$E(w)$ 达到最小值的权向量$w$ ，误差函数可以看成是位于权空间上的一个曲面，因此它的最⼩值出现在权空间中误差函数梯度等于零的位置上：
$$
\bigtriangledown E(w) = 0
$$
此外，通常有多个不等价的驻点，通常会产⽣多个不等价的极⼩值。对于所有的权向量，误差函数的最⼩值被称为全局最⼩值（ golobal minimum）。任何其他的使误差函数的值较⼤的极⼩值被称为局部极⼩值（ local minima）。

* 局部⼆次近似:

  通过讨论误差函数的局部⼆次近似，我们可以更深刻地认识最优化问题，以及各种解决最优化问题的⽅法。公式的梯度下降变为：
  $$
  E(w) \simeq E(w^*) = \frac{1}{2} (w-w^*) ^T \textbf{H} (w- w^*)
  $$
  矩阵H是正定的当且仅当:
  $$
  v^T H v >0,对所有的v \neq 0都成立
  $$

* 使⽤梯度信息：这个梯度信息的使⽤可以⼤幅度加快找到极⼩值点的速度。原因如下所述。在公式给出的误差函数的⼆次近似中， 误差曲⾯由b和H确 定， 它 包 含 了 总共$\frac{W(W+3)}{2}$个独⽴的元素（因为矩阵H是对称的），其中W 是w的维度（即⽹络中可调节参数的总数）。这个⼆次近似的极⼩值点的位置因此依赖于$O(W^2)$个参数，并且我们不应该奢求能够在收集到$O(W^2)$条独⽴的信息之前就能够找到最⼩值。

* 对角近似
  上⾯讨论的Hessian矩阵的⼀些应⽤需要求出Hessian矩阵的逆矩阵，⽽不是Hessian矩阵本⾝。因此，我们对Hessian矩阵的对角化近似⽐较感兴趣。换句话说，就是把⾮对角线上的元素置为零。对于模式n，Hessian矩阵的对角线元素可以写成：
  $$
  \frac{\partial^2 E_n}{\partial w_{ji}^2} = \frac{\partial^2 E_n}{\partial a_{j}^2} z_i^2
  $$
  需要计算这个近似，所需的计算步骤数为$O(W)$，其中W 是⽹络中权值和偏置的总数。对于原始的Hessian矩阵，计算的步骤数为$O(W^2)$。

* Levenberg-Marquardt近似，或者称为外积近似（ outer product approximation）:
  $$
  H \simeq \sum \limits_{n=1}^N b_N b_n^T, b_n = \triangledown a_n = \triangledown y_n
  $$

* 计算Hessian矩阵的精确计算，所需的计算步骤为$O(W^2)$

* ​

