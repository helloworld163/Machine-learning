#### 参数最优化：

我们可以考虑寻找使得选定的误差函数$E(w)$ 达到最小值的权向量$w$ ，误差函数可以看成是位于权空间上的一个曲面，因此它的最⼩值出现在权空间中误差函数梯度等于零的位置上：
$$
\bigtriangledown E(w) = 0
$$
此外，通常有多个不等价的驻点，通常会产⽣多个不等价的极⼩值。对于所有的权向量，误差函数的最⼩值被称为全局最⼩值（ golobal minimum）。任何其他的使误差函数的值较⼤的极⼩值被称为局部极⼩值（ local minima）。

* 局部⼆次近似:

  通过讨论误差函数的局部⼆次近似，我们可以更深刻地认识最优化问题，以及各种解决最优化问题的⽅法。公式的梯度下降变为：
  $$
  E(w) \simeq E(w^*) = \frac{1}{2} (w-w^*) ^T \textbf{H} (w- w^*)
  $$
  矩阵H是正定的当且仅当:
  $$
  v^T H v >0,对所有的v \neq 0都成立
  $$

* 使⽤梯度信息：这个梯度信息的使⽤可以⼤幅度加快找到极⼩值点的速度。原因如下所述。在公式给出的误差函数的⼆次近似中， 误差曲⾯由b和H确 定， 它 包 含 了 总共$\frac{W(W+3)}{2}$个独⽴的元素（因为矩阵H是对称的），其中W 是w的维度（即⽹络中可调节参数的总数）。这个⼆次近似的极⼩值点的位置因此依赖于$O(W^2)$个参数，并且我们不应该奢求能够在收集到$O(W^2)$条独⽴的信息之前就能够找到最⼩值。

* 对角近似
  上⾯讨论的Hessian矩阵的⼀些应⽤需要求出Hessian矩阵的逆矩阵，⽽不是Hessian矩阵本⾝。因此，我们对Hessian矩阵的对角化近似⽐较感兴趣。换句话说，就是把⾮对角线上的元素置为零。对于模式n，Hessian矩阵的对角线元素可以写成：
  $$
  \frac{\partial^2 E_n}{\partial w_{ji}^2} = \frac{\partial^2 E_n}{\partial a_{j}^2} z_i^2
  $$
  需要计算这个近似，所需的计算步骤数为$O(W)$，其中W 是⽹络中权值和偏置的总数。对于原始的Hessian矩阵，计算的步骤数为$O(W^2)$。

* Levenberg-Marquardt近似，或者称为外积近似（ outer product approximation）:
  $$
  H \simeq \sum \limits_{n=1}^N b_N b_n^T, b_n = \triangledown a_n = \triangledown y_n
  $$

* 计算Hessian矩阵的精确计算，所需的计算步骤为$O(W^2)$

* Hessian矩阵的快速乘法：

  我们可以尝试寻找⼀种只需O(W)次操作的直接计算$v^T H$的⾼效⽅法。

#### 神经⽹络的正则化

有其他的⽅式控制神经⽹络的模型复杂度来避免过拟合。根据我们第1章中对多项式曲线拟合问题的讨论，我们看到，⼀种⽅法是选择⼀个相对⼤的M值，然后通过给误差函数增加⼀个正则化项，来控制模型的复杂度。最简单的正则化项是⼆次的，给出了正则化的误差函数，形式为：
$$
\tilde E (w) = E(w) + \frac{\lambda}{2} w^T w
$$

#### 早停止：

在独⽴数据（通常被称为验证集）上测量的误差，通常⾸先减⼩，接下来由于模型开始过拟合⽽逐渐
增⼤。于是，训练过程可以在关于验证集误差最⼩的点停⽌。

⾃由度有效数量开始时很⼩，然后在训练过程中增长，对应于模型复杂度的持续增长。这样，在训练误差达到最⼩值之前停⽌训练就表⽰了⼀种限制模型复杂度的⽅式。

#### 卷积神经网络

* 软权值共享

  降低具有大量权值参数的网络复杂度的一种方法是将权值分组，然后令分组内的权值相等。其中权值的分组倾向于取近似的值。此外，权值的分组、每组权值的均值，以及分组内的取值范围全都作为学习过程的⼀部分被确定。

  可以使用高斯混合模型来得到正则项：
  $$
  \Omega(w) = -\sum\limits_i In(\sum\limits_{j=1}^M \pi_j N(w_i|\mu_j,\sigma^2))
  \\ \tilde E(w) = E(w) + \lambda \Omega(w)
  $$
  总的误差函数关于权值的导数为:
  $$

  $$

  $$
  \frac{\partial \tilde E}{\partial w_i}=\frac{\partial E}{\partial w_i} + \lambda \sum\limits_j \gamma_j(w_i) \frac{(w_i - \mu_j)}{\sigma_j^2}
  $$


#### 混合密度⽹络

如果正向问题涉及到多对⼀的映射，那么逆问题就会有多个解。例如，多种不同的疾病可能会导致相同的症状。数据集是多峰的时候，逆问题的解是很差的。**混合密度网络就是可以使用x这个数据集来估计这个混合概率中所需要的参数。**

如果混合模型中有K个分量，且t有L个分量，那么⽹络就会有K个输出单元激活（记作$a_k^{\pi}$）确定混合系数$\pi_k(x)$，有K个输出（记作$a_k^{\sigma}$）确定核宽度$\sigma_k(x)$，有$K \times L$个输出（记作$a_{kj}^{\mu}$）确定核中⼼$\mu_k(x)$的分量$\mu_{kj}(x)$。⽹络输出的总数为(L+ 2)K，这与通常的⽹络的L个输出不同。通常的⽹络只是简单地预测⽬标变量的条件均值。

#### 贝叶斯神经网络

正则化的最⼤似然⽅法可以看成MAP（ maximum posterior）⽅法，其中正则化项可以被看成先验参数分布的对数。最终的后验概率可以写成：
$$
p(w|D,\alpha, \beta) \propto p(w|\alpha) p(D|w,\beta)
$$
使⽤拉普拉斯近似，我们可以找到对于后验概率分布的⼀个⾼斯近似。为了完成这⼀点，我们必须⾸先找到后验概率分布的⼀个（局部）最⼤值，这必须使⽤迭代的数值最优化算法才能找到。与之前⼀样，⽐较⽅便的做法是最⼤化后验概率分布的对数。



