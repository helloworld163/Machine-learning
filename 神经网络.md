#### 参数最优化：

我们可以考虑寻找使得选定的误差函数$E(w)$ 达到最小值的权向量$w$ ，误差函数可以看成是位于权空间上的一个曲面，因此它的最⼩值出现在权空间中误差函数梯度等于零的位置上：
$$
\bigtriangledown E(w) = 0
$$
此外，通常有多个不等价的驻点，通常会产⽣多个不等价的极⼩值。对于所有的权向量，误差函数的最⼩值被称为全局最⼩值（ golobal minimum）。任何其他的使误差函数的值较⼤的极⼩值被称为局部极⼩值（ local minima）。

* 局部⼆次近似:

  通过讨论误差函数的局部⼆次近似，我们可以更深刻地认识最优化问题，以及各种解决最优化问题的⽅法。公式的梯度下降变为：
  $$
  E(w) \simeq E(w^*) = \frac{1}{2} (w-w^*) ^T \textbf{H} (w- w^*)
  $$
  矩阵H是正定的当且仅当:
  $$
  v^T H v >0,对所有的v \neq 0都成立
  $$

* 使⽤梯度信息：这个梯度信息的使⽤可以⼤幅度加快找到极⼩值点的速度。原因如下所述。在公式给出的误差函数的⼆次近似中， 误差曲⾯由b和H确 定， 它 包 含 了 总共$\frac{W(W+3)}{2}$个独⽴的元素（因为矩阵H是对称的），其中W 是w的维度（即⽹络中可调节参数的总数）。这个⼆次近似的极⼩值点的位置因此依赖于$O(W^2)$个参数，并且我们不应该奢求能够在收集到$O(W^2)$条独⽴的信息之前就能够找到最⼩值。

* 对角近似
  上⾯讨论的Hessian矩阵的⼀些应⽤需要求出Hessian矩阵的逆矩阵，⽽不是Hessian矩阵本⾝。因此，我们对Hessian矩阵的对角化近似⽐较感兴趣。换句话说，就是把⾮对角线上的元素置为零。对于模式n，Hessian矩阵的对角线元素可以写成：
  $$
  \frac{\partial^2 E_n}{\partial w_{ji}^2} = \frac{\partial^2 E_n}{\partial a_{j}^2} z_i^2
  $$
  需要计算这个近似，所需的计算步骤数为$O(W)$，其中W 是⽹络中权值和偏置的总数。对于原始的Hessian矩阵，计算的步骤数为$O(W^2)$。

* Levenberg-Marquardt近似，或者称为外积近似（ outer product approximation）:
  $$
  H \simeq \sum \limits_{n=1}^N b_N b_n^T, b_n = \triangledown a_n = \triangledown y_n
  $$

* 计算Hessian矩阵的精确计算，所需的计算步骤为$O(W^2)$

* Hessian矩阵的快速乘法：

  我们可以尝试寻找⼀种只需O(W)次操作的直接计算$v^T H$的⾼效⽅法。

#### 神经⽹络的正则化

有其他的⽅式控制神经⽹络的模型复杂度来避免过拟合。根据我们第1章中对多项式曲线拟合问题的讨论，我们看到，⼀种⽅法是选择⼀个相对⼤的M值，然后通过给误差函数增加⼀个正则化项，来控制模型的复杂度。最简单的正则化项是⼆次的，给出了正则化的误差函数，形式为：
$$
\tilde E (w) = E(w) + \frac{\lambda}{2} w^T w
$$

#### 早停止：

在独⽴数据（通常被称为验证集）上测量的误差，通常⾸先减⼩，接下来由于模型开始过拟合⽽逐渐
增⼤。于是，训练过程可以在关于验证集误差最⼩的点停⽌。

⾃由度有效数量开始时很⼩，然后在训练过程中增长，对应于模型复杂度的持续增长。这样，在训练误差达到最⼩值之前停⽌训练就表⽰了⼀种限制模型复杂度的⽅式。

#### 卷积神经网络





