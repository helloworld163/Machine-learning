数据缺失的第一个问题就是如何确定缺失数据的机制是否使得观测数据失真。

* 如果导致其遗漏的机制与其值无关，那么就是随机丢失数据

* 我们假设$y$ 是响应变量，$X$ 为$N \times p$ 的输入矩阵，并且假定$X_{obs}$ 表示为$X$中的观测值。并且我们令$Z=(y,X)$ ，$Z_{obs} = (y,X_{obs})$ .假定R为指示矩阵，如果$x_{ij}$ 缺失，则第$ij$ 个元素的值为1，否则为0

  在这里可以表示为
  $$
  Pr(R|Z,\theta) = Pr(R|Z_{obs}, \theta)
  $$
  举个例子，如果一个病人的测量没有被采用，是因为医生认为他病了，则这个观测不是MAR或者MACR。这种情形下，缺失数据的缺失举止导致我们的观测的训练数据得到真实总体扭曲的情形，这种情形下数据插补是很危险的。通常确定特使是否是MACR必须从数据收集的过程中的信息做出判断。对于类别型特征，诊断这个问题是把缺失作为一个新的类别“缺失”。接着对训练数据拟合我们的模型看看“缺失”这个类别是否预测了一些响应变量。

  假设特征完全随机缺失，有一系列方式可以处理： 1. 丢掉含任意缺失的数据的观测 2. 在训练阶段，依赖于具体学习算法来处理缺失数据 3. 训练前插补所有缺失数据。

  方式（1）用在缺失数据相对较少的情形，其它情形应该避免使用。

  至于（2），CART是有效处理缺失数据的一个学习算法，通过代理分割(surrogate splits)（第9.2.4节）。MARS和PRIM采用类似的方式。在广义加性建模中，当在backfitting算法中对某特征的部分残差被光滑，忽略该给定输入特征的所有缺失观测，并且它们的拟合值设为0。因为拟合曲线均值为0（当模型包含截距时），这意味着对缺失的观测赋予平均的拟合值。

  对于大部分学习方法，插补方式（3）是必要的。最简单的策略是用该特征的非缺失数据的均值或中值进行插补。