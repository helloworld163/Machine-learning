$X = (x_1,x_2,\ldots,x_N)$

$$\begin{align*} &  \sum\limits_{i=1}^N \Vert\mathbf  h(x_i) \Vert^2\\  ={} & \sum\limits_{i=1}^N x_i^T (\mathbf{X^T X})^{-1}x_i  \\ ={} &trace[X(X^T X)^{-1} X^T] \\ ={}&  trace[(X^T X)^{-1} X^TX] \\ ={}&p \end{align*}  $$

$$ \beta_*=\mathrm{arg}\;\underset{\beta}{\mathrm{min}}\;E(f(X)-X^T\beta)^2\qquad (7.13) $$

$\mathbf h(x_0)=\mathbf X(\mathbf {X^TX}+\alpha\mathbf I)^Tx_0$

 $$ \begin{align} &E_{x_0}[f(x_0)-E\hat f_\alpha(x_0)]^2  \\ = {} &E_{x_0}[f(x_0)-x_0^T\beta_*]^2+E_{x_0}[x_0^T\beta_*-Ex_0^T\hat\beta_\alpha]^2\ \\={}& \text{Ave[Model Bias]}^2+\text{Ave[Estimation Bias]}^2\qquad (7.14) \end{align} $$

$$ \frac{1}{N}\sum\limits_{i=1}^N\mathrm{Err}(x_i)=\sigma_\varepsilon^2+\frac{1}{N}\sum\limits_{i=1}^N[f(x_i)-E\hat f(x_i)]^2+\frac{p}{N}\sigma_\varepsilon^2\qquad (7.12)$$



## 线性回归模型和最小二乘法

| **原文** | The Elements of Statistical Learning |
| ------ | ------------------------------------ |
| 翻译     | Xiaogang Xu                          |
| 时间     | 2017-2-20                            |

正如第二章介绍的那样，我们有输入向量$X^T = (X_1,X_2,\ldots,X_p)$,而且想要预测实数值的输出$Y$。线性模型有如下形式$$f(X) = \beta_0 + \sum\limits_{j=1}^p X_j\beta_j \qquad(3.1) $$

线性模型要么假设回归函数是线性的，要么假设线性模型是一个合理的近似。在这里$\beta_j$是位置的参数或者系数，变量$X_j$可以有下列不同的来源

$$\hat\beta^\mathrm{ridge} = \mathrm{arg}\min\limits_\beta{\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^p x_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^p \beta_j^2}$$

$$ \begin{align} RSS(\beta) &=\sum\limits_{i=1}^N(y_i-f(x_i))^2 \\  &=\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2\qquad (3.2) \end{align} $$

$\bar y=\frac{1}{N}\sum\limits_1^Ny_i$

$$ RSS(\lambda)=(\mathbf{y}-\mathbf{X}\mathbf{\beta})^T(\mathbf{y}-\mathbf{X}\beta)+\lambda\beta^T\beta \qquad (3.43) $$

$$\begin{align} f(\beta | y) &= \frac{f(\beta, y)}{f(y)} \\ &= \frac{f(\beta,y)}{\int f(y|\beta)f(\beta)} \\ &\sim f(y|\beta)f(\beta) \\ &=\mathrm{Cexp}{-\frac{1}{2\sigma^2}\big[(y-\beta_0-\mathbf X\beta)+\frac{\sigma^2}{\tau^2}\beta'\beta]} \end{align}$$

lasso估计的定义如下

$$\hat \beta^{lasso} = \mathrm{arg} \min\limits_{\beta} \sum\limits_{i=1}^N(y_i - \beta_0 - \sum\limits_{j=1}^p x_{ij}\beta_j)^2 \\\mathrm{subject\quad to} \sum\limits_{j=1}^p |\beta_j| \leq t$$





$$Pr(G=k|\mathrm{X}=x) = \frac{f_k(x)\pi_k}{\sum_{\ell}^{\cal K} f_{\ell}(x)\pi_{\ell}} \qquad(4.7)$$

$$f_{\cal k}(x)=\frac{1}{(2\pi)^{p/2}|\sum_{\cal k}^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^T\sum\limits_k^{-1}(x-\mu_k)} \qquad(4.8)$$

$\begin{array}{ll} log\frac{Pr(G=k\mid X=x)}{Pr(G=\ell\mid X=x)}&=log\frac{f_k(x)}{f*\ell(x)}+log\frac{\pi_k}{\pi_\ell}&=log\frac{\pi_k}{\pi_\ell}-\frac{1}{2}(\mu_k+\mu_\ell)^T\Sigma^{-1}(\mu_k-\mu_\ell)+x^T\Sigma^{-1}(\mu_k-\mu_\ell) \end{array}\qquad(4.9) $$

$$\begin{align}& log\frac{Pr(G=k\mid X=x)}{Pr(G=\ell\mid X=x)}\\ &=log\frac{f_k(x)}{f*\ell(x)}+log\frac{\pi_k}{\pi_\ell}\\&=log\frac{\pi_k}{\pi_\ell}-\frac{1}{2}(\mu_k+\mu_\ell)^T\Sigma^{-1}(\mu_k-\mu_\ell)+x^T\Sigma^{-1}(\mu_k-\mu_\ell) \end{align}\qquad(4.9) $$



这里我们讨论样条基方法来完全避免通过使用最大结点集合造成的结点选择问题。拟合的复杂度由正则化来控制。考虑下面的问题：在所有的二阶连续微分的函数$f(x)$中，找到一个使得惩罚残差平方和最小

$$Rss(f,\lambda)=\sum\limits_{i=1}^N \{y_i - f(x_i) \}^2+ \lambda\int \{f^{''}(t)\}^2 \mathrm{d}{x}$$

其中$\lambda$是固定的光滑参数，第一项衡量的是和数据的近似程度，第二项对函数的曲率进行惩罚，且$\lambda$建立了两者之间的一个平衡。两个特别的例子是：

* $\lambda$=0:$f$可以是任意对数据插值的函数

#### 鲁棒的分类损失函数

-----------

**边缘**指的是 $yf(x)$，与回归中的残差$y-f(x)$起着类似的作用。分类准则$G(x) = sign[f(x)]$ 表明有正边缘

$K$个类别的多项式偏差损失函数

$\begin{align} L(y,p(x)) &= -\sum\limits_{k=1}^K \mathbf{I}(y= \cal G_k)logp_k(x) \\&= -\sum\limits_{k=1}^K \mathbf{I}(y= \cal G_k)f_k(x) + log(\sum\limits_{\cal l = 1}^{\cal K}e^{f_{\cal l}(x)}) \end{align}$

#### 鲁棒的回归损失函数

--------------

在回归问题中，类似指数损失和二项对数似然的关系是平方误差$L(y,f(x))=(y-f(x))^2$ 

平方误差的总体解为 $f(x) = E(Y|x)$ ，而绝对值损失$f(x)=\mathrm{median}(\mathbf{Y}|x)$  



