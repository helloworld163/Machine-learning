### 关联规则

关联规则分析已经作为一个流行的工具被使用在商业数据挖掘。其目标是寻求数据库中最频繁出现的变量$X=(X_1,X_2,…,X_p)$ 的联合值。该技术最常用于二值数据$X_j\in\{0,1\}$ ,在那里它被称为是“购物篮”分析。在此背景下，观测是销售事物，诸如商场收银台的事务等。变量代表的是商场销售的所有商品。对于观测$i$，每个变量$X_j$被赋予二值之一；如果第$j$种商品在交易中售出，则$x_{ij}=1$ ，反之$x_{ij}=0$ 频繁地具有联合值1的那些变量代表频繁地同时购买的商品。这些信息对货架库存，促销商品搭配，价目表设计以及以根据购买模式对消费者进行分类都是十分有用的。

更一般地，关联规则分析的基本目标就是能够为特征向量$X$求解一个原型$X$值的集合$\nu_1,…\nu_L$,使得在这些值上计算的概率密度$Pr(\nu_l)$相对较大。在这个通用的框架下，该问题可以看做是“众数发现”或者是“凸点搜索”。至于用公式表示，这个问题将会变得十分困难。对每一个$Pr(\nu_l)$的一个自然估计是$\mathbf{X}=\nu_l$ 的观测值所占的比例。对于涉及变量较多，且每一个变量可能具有多个值的问题，$\mathbf{X}=\nu_l$的观测的个数对于可靠地估计来说几乎总是太小。因此，为了得到一个易于处理的问题，分析的目标和用于分析的数据的一般性都必须要大为简化。



首要的简化就是需要修改目标。我们要寻找的是相对于样本量或者支持度具有高概率的$\mathbf{X}$空间区域，而不是寻找$Pr(x)$大的$x$值。令$S_j$ 表示第$j$个变量所有可能值的集合，并且令$s_j\in S_j$为这些值的子集。修改后的目标为试图求解变量值的子集$s_1,…,s_p$使得每个变量同时在其对应的子集中取值的概率 :  

$Pr[\bigcap\limits_{j=1}^p(X_j\in s_j)]$

$S_j$是对于每一个变量$X_j$的支持集样本数

$$\widehat Pr\left[ \prod\limits_{k\in \cal K}(\mathbf{Z}_k=1) \right]=\frac{1}{N}\sum\limits_{i=1}^N \prod\limits_{k\in \cal K}z_{ik} $$

$\cal K$为下标集，$\{Z_K|k\in \cal K\}$在这里是项集

在这里的假设下 ，对于每一个变量的取值子集$s_j$ 要么仅仅包括$X_j$的一个值，也就是$s_j=\nu_{0j}$ ；

或者$s_j$包括$X_j$设定取值的整个集合 $s_j=S_j$.这就是将问题化简为求整数$\cal J \subset{1,…,p}$的子集和对应值$v_{0j},j\in \cal J$ 

我们可以应用哑变量(dummy variables)技术将式(14.3)变换成只涉及二值变量的问题。这里假定对每个变量$X_j$的支持集样本数$S_j$是有限的。特别地，创建一个新的变量集合$Z_1,…,Z_K$，每个变量代表一个可以由原始变量$X_1,…,X_p$获得的值$\nu_{lj}$。哑变量的个数$K$是：$K=\sum\limits_{j=1}^p |S_j|$

#### Apriori算法

$$\{ \cal K_l|T(\cal K_l) > t\}\qquad(14.6)$$  

调整阈值$t$使得式(14.6)只包括全部$2^K个可能项集中的一小部分，对于大型数据库，问题(14.6)的解可以有可行的计算而得。

特殊地，对于给定的支持度阈值$t$：

* 基数$|\{\cal K|T(\cal K)>t\}|$ 相对较小
* 任何由$\cal K$ 中项的子集组成的项集$\cal L$ ,必须具有大于或者等于项集$\cal K$ 的支持度，即$\cal L \subseteq \cal K \Rightarrow T(L) \geq T(K)$ 

第一遍扫描数据，计算所有1项集的支持度。删除那些支持度小于阈值的项。第二次扫描对可以通过第一次扫描而保留下来的项生成的所有2项集计算支持度。也就是说，要生成所有满足$|K|=m$的频繁项集，我们仅需要考虑这样的m项集候选，它们的$m$个规模为$m-1$的祖先项集都是频繁的。删除那些支持度小于阈值的2项集。每一遍后继数据扫描只考虑能由前一遍扫描保留项集与第一遍扫描留下的项组成所生成的项集。继续扫描数据，直到由上一步得到的所有候选项集的支持度都小于指定的阈值。对于$|\cal K|$的每个值，Apriori算法只需要扫描一次数据。这是至关重要的，因为我们假定所有数据不能全部装入计算机内存。如果数据足够稀疏，即使对巨大的数据集，该过程也能在合理的时间内终止。

由Apriori算法得到的每个形如式(14.6)的高支持度项集$\cal K(14.6)$产生一组“关联规则”。项$Z_k,k \in \cal K$ 被划分成两个不相交的子集。$A\bigcup B = \cal K$ ,并且记为： $A \Rightarrow B$ 

规则的支持度$T(A\Rightarrow B)$ 是前件和后件并集中观测的比例，是随机选择的购物篮中同时观测到两个项集的概率$Pr(A,B)$的一个估计(14.5)

规则的“置信度”或者“预测度” $C(A\Rightarrow B)$ 是规则的支持度除以其前件的支持度

$$C(A\Rightarrow B) = \frac{T(A\Rightarrow B)}{T(A)}$$ 

它可以看做是$Pr(B|A)$的一个估计。 

“期望置信度” $T(B)$是无条件概率$Pr(B)$的一个估计。

规则的提升度定义为置信度除以期望置信度：

$$L(A \Rightarrow B) = \frac{C(A\Rightarrow B)}{T(B)}$$ ，这也是$$\color{red}{ \frac{Pr(A,B)}{Pr(A)Pr(B)} }$$ 的一个估计

对于每一个规模为$|\cal K|$ 的项集$\cal K$ ，存在$2^{|K|-1} -1$ 个形如$A\Rightarrow (\cal K -A)$ 的规则，其中$A\subset \cal K$ 

整个分析的输出是满足以下约束条件的关联规则(14.7)集合：

$$T(A \Rightarrow B) >t  $$ 和$$C(A\Rightarrow B) >c $$

 关联规则已经成为分析有关于购物篮等大型商用数据库的一个流行工具。即数据可以映射为多维列联表的形式。输出是易于理解和解释的合取规则的形式。

##### 14.2.4 作为有指导学习的无指导学习

设$g(x)$为待估计的未知数据的概率密度，$g_0(x)$ 为用于推理的指定概率密度函数。举例来说，$g_0(x)$可以是变量值域上均匀的密度，其他可能将在下面讨论。假定数据集$x_1,x_2,…,x_N$ 是抽自$g(x)$ 的一个独立同分布的随机样本。一个规模为$N_0$ 的样本可以用蒙特卡洛方法由$g_0(x)$抽取。将这两个数据集合并，并且将质量$w=\frac{N_0}{N+N_0}$ 赋予从$g(x)$得到的样本，将质量$w_0=\frac{N}{N+N_0}$ 赋予从$g_0(x)$ 得到的样本，导致一个从混合密度$\frac{(g(x)+g_0(x))}{2}$ 抽样的随机样本。如果把$Y=1$ 赋予从$g(x)$ 抽取的每个样本点，把$Y=0$ 赋予从$g_0(x)$ 抽取的每个样本点，那么：

$$\begin{align} \mu(x) = E(Y|x) &= \frac{g(x)}{g(x)+g_0(x)} \\ &= \frac{g(x)/g_0(x)}{1+g(x)/g_0(x)}\end{align}$$

可以用有指导学习方法来估计，使用组合的样本$(y_1,x_1),(y_2,x_2),…,(y_{N+N_0},x_{N+N_0})$ 作为训练数据。结果估计$\hat \mu(x)$ 可以转换为$g(x)$的一个估计

$\hat g(x)=g_0(x) \frac{\hat \mu(x)}{1-\hat \mu(x)}$  实际上估计$g(x)$ 的精度非常依赖于特定参考密度的选择。

与独立性的差异可以通过使用下式研究：$$g_0(x)=\prod\limits_{j=1}^p g_j(x_j)$$ 

广义关联法则：
 $$ \widehat {Pr} \bigl(  \bigcap\limits_{j\in \cal J}(X_j \in s_j) \bigr) = \frac{1}{N} \sum\limits_{i=1}^N I\bigl( \bigcap\limits_{j \in \cal J}(x_{ij} \in s_j) \bigr)$$

其中$\{  (X_j \in s_j) \}_{j\in \cal J} $ 被称作是“广义”项集

忍耐规则归纳方法PRIM，PRIM也精确地生成符合式(14.18)形式的规则，但是它为求解具有最大平均目标值的高支持度区域特别设计的，而不是试图在整个数据空间为目标函数建模



### 聚类分析

聚类分析也被称为数据分割，具有多种目标，但是都涉及把一个对象集合分组或者分割为子集或者“簇”，使得每个簇内部的对象之间的相关性比与其他簇中对象之间的相关性更紧密。

* 定量型变量：这种变量定义两者之间的误差为

  $d(x_i,x_i')=l(|x_i-x_i'|)$ 

  聚类可以基于相关性：

  $\sum_j(x_{ij}-\overline{x_i})(x_{i'j}-\overline{x_{i'}})$

  $\sqrt{\sum_j(x_{ij}-\overline{x_i})^2 \sum_j (x_{i'j}-\overline{x_{i'}})^2}$

* 序数型变量：用$\frac{i-1/2}{M}, i=1,…,M $ 将其转化为定量型

* 分类型变量

**组合算法** 

自然的损失：

$$W(C)=\frac{1}{2}\sum\limits_{k=1}^K \sum\limits_{C(i)=k} \sum\limits_{C(i')=k} d(x_i,x_{i'})$$  表征了赋予相同类的观测趋向相互间接近的程度。簇内点分布

$$B(C)=\frac{1}{2}\sum\limits_{k=1}^K \sum\limits_{C(i)=k} \sum\limits_{C(i')\neq k} d(x_i,x_{i'})$$ 

**算法14.1 K-均值聚类**

1. 对给定的簇指派$C$ ，关于$\{ m_1,…,m_K \}$ 对总的簇方差$\min\limits_{C,\{ m_k \}_1^K} \sum\limits_{k=1}^K N_k \sum\limits_{C(i)=k} \| x_i-m_k\|^2 $ 

2. 给定均值的当前集合$\{ m_1,…,m_K \}$ ,通过将每个观测指派到当前最近的簇均值，极小化：

   $C(i)=arg\min\limits_{1 \leq k \leq K} \| x_i - m_k\|^2$

3. 重复1，2，直到指派不再改变

**向量量化** 

图像压缩的例子，在这里聚类的过程就是编码步，而且形心的集合称编码本 

**K中心点**

KNN是分类算法，根据k个周围的进行投票，属于监督学习

Kmeans是聚类算法，属于非监督学习

K中心算法比K-均值的计算密集很多

估计$K^*$ 的基于数据的方法把类内相异度$W_K$ 看做簇个数K的函数。对于$K \in \{ 1,2,…,K_{max} \}$ 分别得到解。

为选择合适的K值，我们可以找到一个识别纽结$K=K^*$ 

$K^*=\underset{K}{argmin} \{  K|G(K) \geq G(K+1)-s_{K+1}' \}$

**分层聚类**

分层聚类产生分层表示，每一层的簇通过合并其次低层的簇而创建。在最底层，每个簇包括一个观测。在最高层只有一个簇，它包括所有的数据。

* 凝聚的（自下而上）
* 分裂的（自上而下）

树状图以图的形式提供分层聚类完整的高度可解释的描述

两个簇之间相异度的度量$d_{SL}(G,H)=\min\limits_{\begin{align}& t' \in H\\ & t \in G \end{align}}d_{tt'}$  这里将组间的相异度定义为最近对的相异度

$d_{CL}(G,H)=\max\limits_{\begin{align}& t' \in H\\ & t \in G \end{align}}d_{tt'}$ 在完全连接的凝聚聚类中，将组类间的相异度定义为最远对的相异度



**分裂聚类**

### 自组织映射SOM

**主成分**

也就是极小化重构误差 $\min\limits_{\mu,\{ \lambda \},\mathbf{V}_q} \sum\limits_{i=1}^N\| x_i-\mu-\mathbf{V}_q \lambda_i \|^2$ 

其中有$\hat \mu=\hat x$ ,$\hat \lambda_i=\mathbf{V}_q^T(x_i-\hat x)$ 



**主曲线和曲面**

曲流形近似，首先为随机变量$X\in \mathbf{IR}^p$ 定义主曲线，然后转向有限数据的情况。令$f(\lambda)$ 为$\mathbf{IR}^p$ 中参数化的光滑曲线，是具有p个坐标的向量函数。

$f(\lambda)=\mathbf{E}(X|\lambda_f(X)=\lambda)$ 

其中$\lambda_f(x)$ 是曲线上离x最近的点。

$f(\lambda)$ 为随机向量X分布的主曲线



#### 独立成分分析和探测性投影寻踪

多元数据一般需要用因子分析的技术，在以识别这些本证源为目标

**本征变量与因子分析**

奇异值分解$\mathbf{X}=\mathbf{UDV^T}$ 有一个本征变量表示。记$\mathbf{S}=\sqrt{N}\mathbf{U}$ 和$\mathbf{A}^T =\mathbf{DV^T}/\sqrt{N}$ ,我们有$X=SA^T$ ,因此$X$ 的每一列是$S$ 的列的线性组合。由于$U$ 是正交的，并且像以前一样假设$X$ 的每一列具有均值0（对于$U$也是如此）。因此$S$的列具有均值0，是不相关的且具有单位方差。

所以我们可以将$SVD$ 或者相关的主成分分析解释为本征模型$X=\mathbf{A}S$ 的一个估计。每个相关变量$X_j$ 表示为一个非相关的单位方差变量$S_{\cal l}$ 的线性展开。但是这并不太令人满意，因为给定任意$p\times p$ 的正交矩阵$\mathbf{R} $ ，我们都可以记$\begin{align} X& =\mathbf{A}S\\ &=\mathbf{AR^TRS} \\&= \mathbf{A^*} S^*\end{align}$ 

可以看出其实存在着许多这样的分解，从而识别任意一个特定的本征变量为唯一的潜在源是不可能的。$SVD$ 分解确实有着这样的特性：任意秩$q<p$ 的截断分解都以最优的方式逼近$\mathbf{X}$ 

在$q<p$ 的情况下，我们有$\mathbf{X}=\mathbf{A}S+\varepsilon $  $A$ 是$p\times q$ 的因子负荷矩阵，$\varepsilon$在这里是均值为0的均值干扰。基本思想是——本征变量是$X_j$ 中公共变化源，并且它们相关结构的原因所在。 模型用极大似然拟合。参数在如下协方差矩阵中：$\sum = \mathbf{AA^T}+\mathbf{D}_{\epsilon}$ 其中$D_{epsilon}=diag[Var(\epsilon_1),…,Var(\epsilon_p)]$ .在这里我们假设用来进行SVD的$S_j$ 是高斯的而且是不相关的。

**独立成分分析** 除了假设$S_i$ 是统计独立而非不相关之外，独立成分分析模型也有着极为相似的形式。通常的，统计独立性决定所有的交叉矩，只要假设$S_i$ 是独立的且非高斯的，我们就可以避开识别的非唯一性问题。

因子分析与主成分分析：他们都是主要用来降低维度的一种手段。

主成分：进行坐标的变换，并且需要使得在保持变量的总方差不变的情况下，使得同时具有最大方差

因子分析：如何用最少的信息丢失，使得众多原始的变量能够浓缩为少数几个因子变量。也就是寻找潜在的能够起到支配作用的变量，是对原始变量的分解而不是重新组合

独立成分分析ICA方法都是基于熵的，密度为$g(y)$ 的随机变量Y的微分熵$H$ 由下式给出

$H(Y)=-\int g(y)log g(y) \mathrm{d}y$ 

在具有相同方差的所有随机变量中，高斯变量的熵最大

随机向量$Y$ 每两个分量之间的互信息$I(Y)$ 是依赖性的一个自然度量

$I(Y)=\sum\limits_{j=1}^p H(Y_j)-H(Y)$ 

求解使得$I(Y)=I(A^TX)$ 极小化的$\mathbf{A}$ 即为求解一个正交变换，该正交变换导致其分量之间的最大独立性。这等价于极小化$Y$ 的各分量熵的总和，相当于最大化背离高斯性。

**另外一种不同的ICA方法**

独立成分有一个联合积密度，所以为求解它们，可以估计它们的积密度。通过把密度估计任务看成一个2-类的分类问题，将问题简化。

观测的数据点被指派为类$G=1$ ，背景样本由一个密度$g_0(x)$ 生成，并且指派为类$G=0$ 

考虑一个双变量问题$X=(X_1,X_2)$ ，以及如下形式的二项模型：

$log\frac{Pr(G=1)}{1-Pr(G=1)}=f_1(a_1^T X)+f_2(a_x^T X)$ 

该分对数的加法模型给出如下形式的数据密度：

$g(X)=g_0(X)\cdot exp\{f_1(a_1^T X)\} \cdot exp\{f_2(a_2^T X)\}$ 

多维定标寻求值$z_1,z_2,…,z_N \in \mathbf{IR}^k$ ，应该极小化应力函数(stress function)

$$S_D(z_1,z_2,…,z_N)=\big[  \sum\limits_{i\neq i'} (d_{ii'}-\|z_i -z_i'\|)^2 \big]^{1/2} $$

也就是最小二乘定标，或者是Kruskal-Shephard定标

其中$d_{ij}=\| x_i -x_j\|$ 

Sammon映射，它极小化

$$\sum\limits_{i \neq i'} \frac{(d_{ii'}-\| z_i-z_i'\|)^2}{d_{ii'}}$$ 

这里更加强调了要保持较小的逐队距离。

**非度量定标** 

极小化应力函数: $$\frac{\sum_{i,i'}[\theta(\|z_i -z_i'\|)-d_{ii'}]^2 }{\sum_{i,i'}d_{i,i'}^2}$$

其中$\theta(\cdot)$ 是任意的一个增函数，我们固定$\theta(\cdot)$ ，通过梯度下降在$d_{ii'}$ 上极小化应力函数









 

 

