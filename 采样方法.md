确定性的近似推断方法是一种近似的方法，包括诸如变分贝叶斯方法和期望传播。我们还可以考虑基于数值采样的近似推断方法。也就是蒙特卡洛方法。

采样⽅法背后的⼀般思想是得到从概率分布$p(z)$中独⽴抽取的⼀组变量$z^{(l)}$，我们可以通过下式来进行：
$$
\hat f = \frac{1}{L} \sum\limits_{l=1} ^L f(z^{(l)})
$$
对于许多模型来说，联合概率分布$p(z)$可以使⽤图模型很容易地确定。在没有观测变量的有向图的情形，从联合概率分布中采样是很容易的，⽅法是使⽤祖先采样⽅法。

算法可以从后验概率分布中正确地采样，因为它对应于从隐含变量和数据变量的联合概率分布中采样然后丢弃那些与观测数据不相符的样本。除了从条件概率分布中采样之外，我们可能也需要从边缘概率分布中采样。如果我们已经有了⼀种从联合概率分布$p(x,v)$中采样的⽅法。

#### 重要采样

与拒绝采样的情形相同，重要采样⽅法的成功严重依赖于采样分布$q(z)$与所求的概率分布$p(z)$的匹配程度。经常出现的情形是$p(z)$变化剧烈，并且⼤部分的质量集中于z空间的⼀个相对较⼩的区域中，此时重要性权重$\{ r_l \}$由⼏个具有较⼤值的权值控制，剩余的权值相对较⼩。

#### 采样与EM算法

蒙特卡罗EM算法的⼀个特定的情形，被称为随机EM（ stochastic EM）。如果我们考虑有限数量的概率分布组成的混合模型，并且在每个E步骤中只抽取⼀个样本时，我们就会⽤到这种算法。这⾥，潜在变量Z描述了K个混合分量中的哪个分量被⽤于⽣成每个数据点。在E步骤中， Z的样本从后验概率分布$p(Z | X, θ^{旧})$中抽取，其中X是数据集。这⾼效地将每个数据点硬性地分配到混合分布中的⼀个分量中。在M步骤中，对于后验概率分布的这个采样的近似被⽤于按照平常的⽅式更新模型的参数。

#### 混合蒙特卡罗算法

正如我们已经注意到的那样， Metropolis算法的⼀个主要的局限是它具有随机游⾛的⾏为，⽽在状态空间中遍历的距离与步骤数量只是平⽅根的关系。在这里的方法基于对物理系统的⼀个类⽐，能够让系统状态发⽣较⼤的改变，同时让拒绝的概率较低。

位置和动量组成的联合空间被称为相空间（ phase space）。对于⼀个⾮零的步长ϵ，蛙跳算法的离散化会在哈密顿动⼒
学⽅程的积分过程中引⼊误差。混合蒙特卡罗（ hybrid Monte Carlo将哈密顿动态系统与Metropolis算法结合在⼀起，因此消除了与离散化过程关联的任何偏差。具体来说，算法使⽤了⼀个马尔科夫链，它由对动量变量r的随机更新以及使⽤蛙跳算法对哈密顿动态系统的更新交替组成。在每次应⽤蛙跳算法之后，基于哈密顿函数H的值，确定Metropolis准则，确定⽣成的候选状态被接受或者拒绝。因此，如果$(z, r)$是初始状态， $(z^∗, r^∗)$是蛙跳积分后的状态，那么候选状态被接受的概率为:
$$
\min(1, exp\{ H(z,r)- H(z^*, r^*) \})
$$
如果蛙跳积分完美地模拟了哈密顿动态系统，那么每个这种候选状态都会⾃动地被接受，因为H的值会保持不变。⼀种估计划分函数⽐值的⽅法是使⽤概率分布的重要采样，这个概率分布的能量函数为$G(z)$.

#### 切⽚采样

我们已经看到， Metropolis算法的⼀个困难之处是它对于步长的敏感性。如果步长过⼩，那么由于随机游⾛⾏为，算法会很慢。⽽如果步长过⼤，那么由于较⾼的拒绝率，算法会很低效。切⽚采样（ slice 提供了⼀个可以⾃动调节步长来匹配分布特征的⽅法。

我们可以通过从$\hat p(z, u)$中采样，然后忽略u值的⽅式得到$p(z)$的样本。 

