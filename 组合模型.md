经常发现的⼀件事情是，我们可以通过以某种⽅式将多个模型结合到⼀起的⽅法来提升性能，⽽不是独⽴
地使⽤⼀个单独的模型。

* 委员会⽅法的⼀个重要的变体，被称为提升⽅法（ boosting）。这种⽅法按顺序训练多个模型，其中⽤来训练⼀个特定模型的误差函数依赖于前⼀个模型的表现。
* 另⼀种形式的模型组合是选择⼀个模型进⾏预测，其中模型的选择是输⼊变量的⼀个函数。因此不同的模型⽤于对输⼊空间的不同的区域进⾏预测。这种⽅法的⼀种⼴泛使⽤的框架被称为决策树（ decision tree）.

#### 贝叶斯模型平均

模型包含⼀个⼆值潜在变量z，它表⽰混合分布中的哪个分量⽤于⽣成对应的数据点。因此，模型通过联合概率分布 $p(x, z)$进⾏具体化.这个在h上的求和式的意义是，只有⼀个模型⽤于⽣成整个数据集， h上的概率分布仅仅反映了我们对于究竟是哪个模型⽤于⽣成数据的不确定性。

构建⼀个委员会的最简单的⽅法是对⼀组独⽴的模型的预测取平均。这种观点考虑偏置和⽅差之间的折中，它将模型的误差分解为偏置分量和⽅差分量。

可以证明，委员会误差的期望不会超过各个分量模型的期望误差.

#### 提升⽅法

提升⽅法是⼀种很强⼤的⽅法，它将多个“基”分类器进⾏组合，产⽣⼀种形式的委员会，委员会的表现会⽐任何⼀个基分类器的表现好得多。

提升⽅法和委员会⽅法（例如上⾯讨论的打包⽅法）的主要不同在于，基分类器是顺序训练的，每个基分类器使⽤数据集的⼀个加权形式进⾏训练，其中与每个数据点相关联的权系数依赖于前⼀个分类器的表现。

#### 基于树的模型

有许多简单但⼴泛使⽤的模型，它们将输⼊空间划分为超⽴⽅体区域，超⽴⽅体的边与坐标轴对齐，然后为每个区域分配⼀个简单的模型（例如，⼀个常数）。这些模型可以被看成⼀种模型组合⽅法，其中只有⼀个模型对于输⼊空间中任意给定点的预测起作⽤。给定⼀个新的输⼊x，选择⼀个具体的模型的过程可以由⼀个顺序决策的过程描述，这个过程对应于对⼀个⼆叉树（每个节点划分为两个分⽀的树）.

像CART这种树模型的可以由⼈类进⾏表述这⼀性质通常被视为它的⼀个重要的优点。然⽽，在实际应⽤中，学习到的特定的树结构对于数据集的细节⾮常敏感。

另⼀种得到专家层次混合模型的⽅法是从标准的⾮条件密度模型（例如⾼斯分布）的概率混合开始，将分量概率密度替换为条件概率分布。这⾥，我们考虑线性回归模型的混合以及logistic回归模型的混合.



