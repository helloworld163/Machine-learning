对于许多应⽤来说，独⽴同分布的假设不成⽴。这⾥，我们考虑这样的数据集中的⼀个重要的类型，即描述了顺序数据的数据集。⽅便起见，我们有时会⽤“过去”观测或者“未来”观测来称呼某个观测。然⽽，本章中研究的模型同样适⽤于所有形式的顺序数据，⽽不仅仅是时间序列数据。

#### 马尔科夫模型

如果我们现在假设右侧的每个条件概率分布只与最近的⼀次观测有关，⽽独⽴于其他所有之前的观测，那么我们就得到了⼀阶马尔科夫链（ first-order Markov chain）：
$$
p(x_1,\dots ,x_N) = p(x_1) \sum\limits_{n=2}^N p(x_n|x_{n-1})
$$
在这种模型的⼤部分应⽤中，条件概率分布$p(x_n | x_{n−1})$被限制为相等的。我们可以类似地考虑扩展到M阶马尔科夫链，其中⼀个特定的变量依赖于前M个变量。

对于连续变量来说，我们可以使⽤线性⾼斯条件概率分布，其中每个结点都是⼀个⾼斯概率分布，均值是⽗结点的⼀个线性函数。这被称为⾃回归（ autoregressive）模型或者AR模型。另⼀种⽅法是为
$$
p(x_n | x_{n−M}, \dots , x_{n−1})
$$
使⽤参数化的模型，例如神经⽹络。这种⽅法有时被称为抽头延迟线（ tapped delay line），因为它对应于存储（延迟）观测变量的前⾯M个值来预测下⼀个值。

对于每个观测$x_n$，我们引⼊⼀个对应的潜在变量$z_n$（类型或维度可能与观测变量不同）。我们现在假设潜在变量构成了马尔科夫链，得到的图结构被称为状态空间模型（ state space model）。

如果潜在变量是离散的，那么我们得到了隐马尔科夫模型（ hidden Markov model）或者HMM；如果潜在变量和观测变量都是⾼斯变量（结点的条件概率分布对于⽗结点的依赖是线性⾼斯的形式），那么我们就得到了线性动态系统（ linear dynamical system）。

#### 隐马尔科夫模型

隐马尔科夫模型的晶格图，状态转移图等。在隐马尔科夫模型的情形，这个步骤修改如下。⾸先我们选择初始的潜在变量$z_1$，概率由参数πk控制，然后采样对应的观测$x_1$。现在我们使⽤已经初始化的$z_1$的值，根据转移概率$p(z_2 | z_1)$来选择变量$z_2$的状态。从⽽我们以概率$A_{jk}$选择$z_2$的状态k，其中$k = 1, \dots , K$。⼀旦我们知道了$z_2$，我们就可以对$x_2$采样，从⽽也可以对下⼀个潜在变量$z_3$采样，以此类推。这是有向图模型的祖先采样的⼀个例⼦。

**EM算法**

E步骤的⽬标是⾼效地计算$\gamma (z_n)$和$\xi(z_{n−1}, z_n)$;在M步骤中，我们关于参数$\theta = \{ \pi ,A,\Theta \}$最⼤化
$$
Q(\theta, \theta^{旧})
$$
**缩放因子**

我们预计这个量在数值计算上可以表现良好，因为对任意n值，它都是K个变量上的⼀个概率分布。为了将缩放的alpha变量与原始的alpha变量关联起来，我们引⼊缩放因⼦，它由观测变量上的条件概率分布定义：
$$
c_n = p(x_n | x_1, \dots, x_{n-1})
$$
然后我们可以将$\alpha$的递归⽅程（13.36）转化为$\hat \alpha$的递归⽅程，也就是重新定义缩放的变量。

#### 线性动态系统

相当于隐变量为高斯变量。线性动态系统是⼀个线性⾼斯模型，因此在所有变量上的联合概率分布以及边缘分布和条件分布都是⾼斯分布。它遵循下⾯的事实：单独地概率最⼤的潜在变量值组成的序列与概率最⼤的潜在变量序列相同。因此对于线性动态系统，⽆需考虑与维特⽐算法类似的算法。由于模型的条件概率分布是⾼斯分布，因此我们可以将转移分布和发射分布写成⼀般的形式：
$$
p(z_n | z_{n-1}) = N(z_n | A z_{n-1}, \Gamma) \\ 
p(x_n | z_n) = N(x_n | Cz_n, \Sigma)
$$
我们可以将Kalman滤波的过程看成下⾯的过程：⾸先做出后续的预测，然后使⽤新的观测来修正这些预测。Kalman滤波的⼀个重要应⽤是跟踪。

#### 粒⼦滤波

对于没有线性⾼斯分布的动态系统，例如使⽤⾮⾼斯发射概率密度的动态系统，为了得到⼀个可以计算的推断算法，我们使⽤采样算法。特别地，我们可以使⽤采样-重要性-重采样⽅法，得到⼀个顺序的蒙特卡罗算法，被称为粒⼦滤波。