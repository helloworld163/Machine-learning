记$h_m(X):\mathbf{IR}^p\rightarrow \mathbf{IR}$ 为 $X$ 的第$m$ 个变换，$m=1,…,M$ 然后，建立$X$ 的线性基展开模型：

$f(X)=\sum\limits_{m=1}^M \beta_m h_m(X)$ .该方法的有点就是一旦确定了基函数$h_m$ ，则模型在这些新变量上是线性的，并且拟合过程与以前一样。

快速检查证实参数的计算：$(A个区域) \times (B个参数/区域) - （C个纽结）\times (D个约束/纽结)$ 

**自然三次样条**

具有$K$ 个纽结的自然三次样条用$K$ 个基函数来表示。我们可以从三次样条的一个基函数开始，并且通过边界约束来推导出归约的基函数。把基函数的所有$p$ 个向量组合在一个大向量$h(X)$ 中，从而模型简化为$h(X)^T\theta$ 

参数的总数为$df=1+\sum_{j=1}^p df_j$ 

##### 过滤和特征提取

首选构造了一个$p \times M$ 的基矩阵$\mathbf{H}$ ，并且将特征向量x变成新的特征向量$x^* =\mathbf{H}^Tx$ ，然后特征的这些过滤之后的版本用作学习过程的输入：在上例中，学习过程是线性逻辑斯蒂回归。

对于信号和图像处理，一种流行的方法是通过小波变换$x^* = H^T x$ 对原始的特征进行变换，然后使用$x^*$ 作为神经网络的输入。小波可以有效地捕捉离散跃变或者强势。而神经网络是为预测变量构造其特征的非线性函数的有力工具



##### 光滑样条

通过使用最大纽结集，它完全避免了纽结选择问题。拟合的复杂性被正则化控制。

考虑下面的问题，在所有具有二阶连续导数的函数$f(x)$ 中，找出一个函数，它极小化罚残差的平方和：

$ RSS(f,\lambda) = \sum\limits_{i=1}^N \{ y_i - f(x_i) \}^2 + \lambda \int \{ f"(t) \}^2 \mathrm{d}t \qquad(5.9)$ 

其中的$\lambda$ 是固定的光滑参数，第一项度量的是与数据的邻近性，而第二项惩罚函数的曲率，并且$\lambda$ 建立二者之间的权衡。两种特殊的情况是：

* $\lambda$ =0:$f$ 可以是对数据插值的任意函数

* $\lambda = \infty$:简单的最小二乘方直线拟合，因为没有二阶导数

   这些都是从非常粗糙到非常光滑的变化（线性的是最光滑的）

可以证明式子(5.9)具有一个显式的，有限维的，唯一的最小化，它就是自然三次样条

可以把它的解写为： $f(x)=\sum_{j=1}^N N_j(x)\theta_j$ 其中$N_j(x)$ 是表示该族自然样条的基函数的$N$ 维集合。

所以该准则归约成：$RSS(\theta,\lambda)=(y-N\theta)^T (y-N\theta) + \lambda \theta^T \Omega_N \theta$ 

可以计算出解为 $\hat \theta = (N^T N + \lambda \Omega_N)^{-1} N^T y$  也就是广义的岭回归

拟合的光滑样条由下式给出： $\hat f(x) = \sum\limits_{j=1}^N N_j(x) \hat \theta_j$

设$B_{\xi}$  是$M$ 个三次样条基函数的$N\times M$ 矩阵，在$N$ 个训练点$x_i$ 上求值，具有纽结序列$\xi$ 

则拟合样条值向量由下式给出：$\hat f = B_{\xi}( B_{\xi}^T B_{\xi})^{-1} B_{\xi}^T y = \mathbf{H}_{\xi} y$

其中线性算子$\mathbf{H}_{\xi}$  是一个投影算子，在统计学上也称为 帽矩阵(hat matrix)

其中$M=trace(H_{\xi})$ 给出了投影空间的维数，它也是基函数的个数。

$df_{\lambda} = trace(S_{\lambda})$ 为光滑样条的有效自由度。可以通过这个条件推出对应的$\lambda$ 



$S_{\lambda} = (I + \lambda K)^{-1} $  其中$\mathbf{K}$ 不依赖于$\lambda$ ，又由于$\hat f = S_{\lambda}y$ 

我们需要解$\min\limits_f(y-f)^T (y-f) + \lambda f^T K f$  

其中$K$ 是罚矩阵

* 本征向量不受$\lambda $ 变化的影响，因而被$\lambda$ 索引的整个光滑样条族(对于一个特定的序列$x$ )具有相同的本征向量
* $S_{\lambda} y =\sum_{k=1}^N  u_k \rho_k(\lambda) < u_k,y>$ ，从而光滑样条通过关于基$\{ u_k\}$ 分解y，并且使用$\rho_k(\lambda)$ 微分地收缩贡献来进行操作。光滑样条被称为是收缩光滑法，而回归样条是投影光滑法
* $u_k$ 的序列，按照$\rho_k(\lambda)$ 的递减排列，看起来增加了复杂度，复杂度越高，收缩越多
* 前两个本征值总是1，并且它们对应于x上线性函数的二维本征空间，永远不被收缩
* 本征值$\rho_k(\lambda) = 1/(1+\lambda d_k)$ 是罚函数$K$ 的本征值$d_k$ 的逆函数，被$\lambda$ 调节，$\lambda$ 控制$\rho_k(\lambda)$ 递减到0的速率。$d_1=d_2=0$ 并且线性函数没有罚
* 可以使用基向量$u_k$ 对光滑样条进行重新参数化
* $df_k=trace(\mathbf{S}_{\lambda}) = \sum_{k=1}^N \rho_k(\lambda)$ 



##### 光滑参数的自动选择

**固定自由度** 

对于光滑样条，由于$df_{\lambda} = trace(S_{\lambda})$ 是在$\lambda$上单调的，我们可以反转该联系，并且固定$df$ 来确定$\lambda$ 。

**偏差—方差权衡** 

$Y = f(X)+ \epsilon$ 

$$f(X) = \frac{\sin(12(X+0.2))}{X+0.2}$$ 

综合的平方预测误差（EPE）将偏倚和方差组合在一个公式中：

$EPE(\hat f_{\lambda}) = E(Y-\hat f_{\lambda}(X))^2 = Var(Y) + E[Bias^2(\hat f_{\lambda}(X))+Var(\hat f_{\lambda}(X))] =\sigma^2 +MSE(\hat f_{\lambda})$ 

**无参逻辑斯蒂回归** 

这里我们考虑具有单个量化输入变量$X$ 的逻辑斯蒂回归，模型是

$$log\frac{Pr(Y=1 | X=x)}{Pr(Y=0|X=x)} = f(x)$$  $\Rightarrow$ $$Pr(Y=1|X=x) = \frac{e^{f(x)}} {1+e^{f(x)}}$$ 

构造罚对数似然准则：
$$\begin{align} \cal l(f;\lambda) &=\sum\limits_{i=1}^N [y_i logp(x_i) + (1-y_i) log(1-p(x_i))]-\frac{1}{2}\lambda \int \{ f''(t) \}^2 dt \\&=[y_if(x_i)-log(1+e^{f(x_i)})]-\frac{1}{2}\lambda \int \{ f''(t) \}^2 dt \end{align}$$











**自由度和光滑矩阵** 

如何为光滑样条选择 $\lambda$ ，这里讨论预先设定光滑量的直观方法

具有预先选定$\lambda$ 的光滑样条是 linear smoother的一个例子

$\hat f = N(N^T N+\lambda \Omega_N)^{-1}  N^T y = S_{\lambda}y$ 

拟合又是在$y$ 上线性的，并且有限线性算子$S_{\lambda}$ 称做光滑子矩阵。 $S_{\lambda}$ 仅仅依赖于$x_i,\lambda$ 



假定$X \in {IR}^2$ 有表示坐标$X_1$ 的函数基$h_{1k}(X_1)，k=1,…,M_1$ 并且类似地对坐标$X_2$ 有$M_2$ 个函数$h_{2k}(X_2)$ 的集合，则由$g_{jk}(X) = h_{1j}(X_1) h_{2k}(X_2)，j=1,…,M_1,k=1,…, M_2$ 

由此可以定义二维函数: $g(X) = \sum\limits_{j=1}^{M_1} \sum\limits_{k=1}^{M_2} \theta_{jk} g_{jk} (X)$ 

图5.10显示使用B样条的张量积基。 基的维数将要呈现指数增长

一维光滑样条也能够拓广到高维。假定有$y_i和x_i$ ，建立问题

$$\min\limits_f \sum\limits_{i=1}^N \{ y_i-f(x_i) \}^2 + \lambda \mathbf{J}[f]$$ 

径向基函数：$h_j(x) = \eta(\| x-x_j \|)$  



##### 正则化和再生核希尔伯特空间

正则化问题一般具有如下的形式:

$\min\limits_{f\in \cal H} \bigl[  \sum_{i=1}^N L(y_i,f(x_i)) + \lambda J(f)  \bigr]$

$J(f)$ 是罚泛函，相当一般的罚泛函是 $$J(f) = \int_{IR^d} \frac{|\widetilde{f}(s)|^2}{\widetilde{G}(s)}ds$$ 

其中$\widetilde{f}$ 表示的是$f$ 的傅里叶变换，而$\widetilde{G}$ 是一个正函数，随着$\| s\| \rightarrow \infty $ 递减到0 

 





  

