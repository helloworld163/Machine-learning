MARS是回归的自适应过程，并且非常适合高维的问题。可以看成是逐步线性回归的推广或者是CART方法使得改进回归设定中的表现。

* 逐步回归分析是从诸多的影响变量中选取一些变量作为自变量。按照其对y的作用大小，显著程度大小，从大到小地逐个引入回归方程。引入回归方程中的变量也可能被剔除。引入一个变量和剔除一个变量都是逐步回归的一步。
* CART：分类回归树。GINI指数：总体内包含的类别越杂乱，GINI指数就越大。最好的划分就是使得GINI最小的划分。

MARS采用的是形如$(x-t)_+$ 和$(t-x)_+$ 的分段线性基函数的展开。

例如：
$$
\ (x-t)_+= \begin{cases}   -t &\mathbf{if} x>t  \\ 0 & otherwise  \end{cases} and \quad (t-x)_+= \begin{cases} -x & \mathbf{if}x>t   \\ 0 & otherwise  \end{cases}
$$
每个函数都是分段线性的，在值t的地方有一个结点。这两个函数称为反射对

对于每个输入$X_j$ ，取结点为每个输入的观测值

采取如下模型：
$$
f(X) = \beta_0 + \sum\limits_{m=1}^M  \beta_m h_m(X)
$$
每一步通过将函数$h_m$ 与C中的某个反射对的所有乘积看成是新的基函数。加入：
$$
\hat \beta_{M+1} h_l(X) \cdot (X_j -t)_+ + \hat \beta_{M+2} h_l(X)\cdot (t-X_j)_+ ,h_l \in \cal M
$$
在模型的项构造时有一个约束：每个输入在一个乘积中至多可以出现一次。这预防了输入的高阶幂的形成，这在参数空间的边界会有剧烈的增大或下降。这样的幂次可以通过分段线性函数以一种更稳定的方式来近似。

MARS过程中的一个有用选项是在交叉项的阶数上设置上界。举个例子，可以设置上界为2，允许成对的分段线性函数的乘积，但是不允许3次或更高次的乘积。这个可以帮助最终模型的解释。含有上界得到加性模型。

