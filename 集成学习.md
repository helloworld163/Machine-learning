* Boosting:训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果。其中的adaboost就是其中的一个典型。

* Bagging：典型的是使用自助法的抽样方法从数据集中得到子训练集，对所有基模型预测的结果进行综合产生最终的预测结果，在这里采样的方法可以不只是限于自助法，但是需要的就是他们之间是没有关系的随机采样，在这一点上只有自助法是比较典型的。

* stacking:stacking：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。

  训练集和测试集都需要用训练好的基模型进行更新。主要的优点在于其能够一定程度上防止过拟合现象。

  ​

模型的偏差是一个相对来说简单的概念：训练出来的模型在训练集上的准确度。

我们认为方差越大的模型越容易过拟合

模型在一定的程度上依旧是随机变量。定义随机变量的值的差异是计算方差的前提条件。过拟合的现象还是指的是其泛化能力太差。设样本容量为n的训练集为随机变量的集合(X1, X2, ..., Xn)，那么模型是以这些随机变量为输入的随机变量函数（其本身仍然是随机变量）：F(X1, X2, ..., Xn)。

不管是bagging还是boosting，其都是一个基模型的权重相加：
$$
E(F)=E(\sum\limits_{i}^{m} \gamma_i \cdot f_i) = \sum\limits_i^m \gamma_i \cdot E(f_i)
$$

* bagging的偏差和方差：
  $$
  E(F)=\mu\\
  Var(F) = \sigma^2 \cdot \rho + \frac{\sigma ^2 \cdot (1-\rho)}{m}
  $$
  同时，整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。另外，在此我们还知道了为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。**在其中可以看到模型的方差是一直存在的，没有办法消除**

  Random Forest是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。Random Fores中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。

* boosting的偏差和方差

  对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对boosting化简公式为：
  $$
  E(F) = \gamma \cdot \sum\limits_i^m E(f_i)\\
  Var(F) = m^2 \cdot \gamma^2 \cdot \sigma^2
  $$
  因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。通过观察整体方差的表达式，我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。

  在增加m的时候，虽然准确率可能会上升，但是相应的方差也增加了，导致防止过拟合的能力下降了。

* boosting可以将其写为迭代的形式:$F^i(x) = F^{i-1}(x)  + h_i(x)$ 

  ​

  ​

