在概率模型的应⽤中，⼀个中⼼任务是在给定观测（可见）数据变量X的条件下，计算潜在变量Z的后验概率分布$p(Z | X)$，但是由于各种原因的存在，我们无法精确计算，所以我们需要借助近似的方法。

根据近似⽅法依赖于随机近似还是确定近似，⽅法⼤体分为两⼤类。这些⽅法基于对后验概率分布的解析近似，例如通过假设后验概率分布可以通过⼀种特定的⽅式分解，或者假设后验概率分布有⼀个具体的参数形式，例如⾼斯分布。对于这种情况，这些⽅法永远⽆法⽣成精确的解。

#### 变分推断

我们可以引⼊泛函的导数（ functional derivative）的概念，它表达了输⼊函数产⽣⽆穷⼩的改变时，泛函的值的变化情况。限制近似概率分布的范围的⼀种⽅法是使⽤参数概率分布$q(Z | \omega)$，它由参数集合$\omega$控制。这样，下界L(q)变成了$\omega$的函数。

我们只需考虑所有隐含变量和可见变量上的联合概率分布的对数，然后关于所有其他的因⼦$\{q_i \}$取期望即可，其中$i \neq j$。⾸先，恰当地初始化所有的因⼦$q_i(Z_i)$然后在各个因⼦上进⾏循环，每⼀轮⽤⼀个修正后的估计来替换当前因⼦。这个修正后的估计由公式（10.9）的右侧给出，计算时使⽤了当前对于所有其他因⼦的估计。算法保证收敛，因为下界关于每个因⼦$q_i(Z_i)$是⼀个凸函数。

两种形式的Kullback-Leibler散度都是散度的alpha家族（ alpha family）的成员。

**确定分量的数量**

如果我们有⼀个由K个分量组成的混合模型，那么每个参数设置都是K!个等价设置中的⼀个。那么基于最⼩化$KL(q ∥ p)$的变分推断会倾向于在某⼀个峰值的邻域内近似这个分布，⽽忽视其他的峰值。由于等价的峰值具有等价的预测分布，因此只要我们考虑⼀个具有具体的数量K个分量组成的模型。如果我们项⽐较不同的K值，那么我们需要考虑这种多峰性。⼀个简单的近似解法是当我们进⾏模型⽐较和平均时，在下界中增加⼀项$ln K!$

