相关向量机（ relevance vector machine）（ RVM），基于贝叶斯⽅法，提供了后验概率的输出，并且通常能产⽣⽐SVM更稀疏的解。

引⼊边缘（ margin）的概念，这个概念被定义为决策边界与任意样本之间的最⼩距离。

**与logistic回归的关系**
与线性可分的情形⼀样，对于线性不可分的概率分布，我们可以⽤最⼩化正则化的误差函数的⽅法重新表⽰SVM。这也使得我们能够强调与logistic回归模型之间的相似性和差别。

我们看到使⽤独⽴的分类器进⾏决策会产⽣不相容的结果，其中⼀个输⼊会同时被分配到多个类别中。这个问题有时可以这样解决：对于新的输⼊x，使⽤下式做预测：
$$
y(x) = \max\limits_k y_k(x)
$$
“1对剩余”⽅法的另⼀个问题是训练集合不平衡。例如，如果我们有10个类别，每个类别的训练数据点的数量相同，那么⽤于训练各个独⽴的分类器的训练数据由90%的负例和仅仅10%的正例组成，从⽽原始问题的对称性就消失了。

另⼀种⽅法是在所有可能的类别对之间训练 $\frac{K(K-1)}{2}$个不同的⼆分类SVM，然后将测试数据点分到具有最⾼“投票数”的类别中去。这种⽅法有时被称为“1对1”（ one-versus-one）。

**回归问题的SVM**
我们现在将⽀持向量机推⼴到回归问题，同时保持它的稀疏性。在简单的线性回归模型中，我们最⼩化⼀个正则化的误差函数。为了得到稀疏解，⼆次误差函数被替换为⼀个$\varepsilon$-不敏感误差函数。如果预测y(x)和⽬标t之间的差的绝对值⼩于$\varepsilon$，那么这个误差函数给出的误差等于零，其中$\varepsilon$ > 0。



